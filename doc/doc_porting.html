<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html  PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>RNP Documentation -- Porting to heavy duty numerical packages</title>
<style type="text/css">@import url(rnp.css);</style>
</head>
<body>


<h1>Porting to heavy duty numerical packages</h1>

<p>Many of the routines are taken directly from Fortran packages, with the interfaces kept fairly close to the original, so porting to an industrial strength, optimized numerical library should be straightforward.
Below is listed the original libraries from which each module originated.</p>

<p>In many instances, simply optimizing the TBLAS layer should afford substantial improvements in performance.
This can be done relatively simply by providing explicit template specializations of TBLAS functions.
Doing this will not achieve performance on par with optimized LAPACK libraries since all blocking has been stripped from the LAPACK routines.
Future developments may restore some blocking functionality.</p>

<p>The BLAS and LAPACK bindings to the core API are being added right now.
In the near future, using vendor provided versions of these libraries will be as simple as defining the preprocessor macros <code>RNP_HAVE_BLAS</code> and <code>RNP_HAVE_LAPACK</code>.
Support currently exists for most of level 1 BLAS, and complex <code>zgemv</code> and <code>zgemm</code> for non-transposed matrices.</p>

<h2>Library origins</h2>

<ul>
<li>IO &ndash; None.</li>
<li>Random &ndash; From LAPACK. Any respectable pseudorandom number generator should be easily substitutable.</li>
<li>TBLAS &ndash; From official BLAS. Note that all interfaces are identical to the originals, except for character job options, which were converted to template parameters in the same order as the original.</li>
<li>Sparse &ndash; None. The matrix format is generic compressed row or column format, so most sparse matrix libraries can handle this.</li>
<li>LinearSolve &ndash; From LAPACK. Follows the LAPACK interface closely.</li>
<li>Eigensystems &ndash; From LAPACK; a direct translation, but removing the character job switches and info return. Note that Eigensystems_jacobi follows the LAPACK interface, but requires no workspace.</li>
<li>Generalized Eigensystems &ndash; From LAPACK; follows the LAPACK interface like Eigensystems.</li>
<li>Iterative Linear Solvers &ndash; Due to the diversity of implementation and interfaces of iterative linear solvers, porting these are not very straightforward. Most iterative linear solvers support the same set of options, so it would simply be a matter of rewriting the function calls. In particular, most solvers use the convention that the matrix multiplication routine returns the result into a new vector, while the preconditioner does not, which is the convention used in RNP.</li>
<li>Iterative Generalized Eigensystems &ndash; To the author&rsquo;s knowledge, there are only a handful of methods that can deal with the fully general case, and none of them allow for arbitrary eigenvalue sorting like RNP. Those that come close are ARPACK (or ARPACK++ for C++) and JDQZ (which is the method from which the RNP implementation was derived). The Matlab JDQZ supports many more options than what is implemented here, but it is unlikely that anyone prototypes in C++ only to use Matlab in the end. In that respect, they are probably stuck with RNP. The interface to ARPACK++ does not require a linear solver, and only certain predefined eigenvalue targets are permitted. Also, preconditioning is not handled explicitly, so substantial consultation of the documentation may be required.</li>
</ul>

</body>
</html>
